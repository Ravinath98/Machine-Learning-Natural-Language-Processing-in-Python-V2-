{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a9c25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like', 'dogs!.']\n"
     ]
    }
   ],
   "source": [
    "x=\"I like dogs!.\"\n",
    "print(x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5389061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19453479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RAVI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b83a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db479228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8287700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a9ffbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber',\n",
       " 'alle',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'alles',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ander',\n",
       " 'andere',\n",
       " 'anderem',\n",
       " 'anderen',\n",
       " 'anderer',\n",
       " 'anderes',\n",
       " 'anderm',\n",
       " 'andern',\n",
       " 'anderr',\n",
       " 'anders',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'bei',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bist',\n",
       " 'da',\n",
       " 'damit',\n",
       " 'dann',\n",
       " 'der',\n",
       " 'den',\n",
       " 'des',\n",
       " 'dem',\n",
       " 'die',\n",
       " 'das',\n",
       " 'dass',\n",
       " 'daß',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'denselben',\n",
       " 'desselben',\n",
       " 'demselben',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'dasselbe',\n",
       " 'dazu',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deinen',\n",
       " 'deiner',\n",
       " 'deines',\n",
       " 'denn',\n",
       " 'derer',\n",
       " 'dessen',\n",
       " 'dich',\n",
       " 'dir',\n",
       " 'du',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'durch',\n",
       " 'ein',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einig',\n",
       " 'einige',\n",
       " 'einigem',\n",
       " 'einigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'er',\n",
       " 'ihn',\n",
       " 'ihm',\n",
       " 'es',\n",
       " 'etwas',\n",
       " 'euer',\n",
       " 'eure',\n",
       " 'eurem',\n",
       " 'euren',\n",
       " 'eurer',\n",
       " 'eures',\n",
       " 'für',\n",
       " 'gegen',\n",
       " 'gewesen',\n",
       " 'hab',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'ich',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihren',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'euch',\n",
       " 'im',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'ins',\n",
       " 'ist',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedes',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'kann',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'keines',\n",
       " 'können',\n",
       " 'könnte',\n",
       " 'machen',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mit',\n",
       " 'muss',\n",
       " 'musste',\n",
       " 'nach',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'ob',\n",
       " 'oder',\n",
       " 'ohne',\n",
       " 'sehr',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'ihnen',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollte',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " 'über',\n",
       " 'um',\n",
       " 'und',\n",
       " 'uns',\n",
       " 'unsere',\n",
       " 'unserem',\n",
       " 'unseren',\n",
       " 'unser',\n",
       " 'unseres',\n",
       " 'unter',\n",
       " 'viel',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'während',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'warst',\n",
       " 'was',\n",
       " 'weg',\n",
       " 'weil',\n",
       " 'weiter',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wenn',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'will',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirst',\n",
       " 'wo',\n",
       " 'wollen',\n",
       " 'wollte',\n",
       " 'würde',\n",
       " 'würden',\n",
       " 'zu',\n",
       " 'zum',\n",
       " 'zur',\n",
       " 'zwar',\n",
       " 'zwischen']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1cba4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords.words('sinhala')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2f9cadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'bengali', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "#availble languages\n",
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b985907",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cad8640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ran\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer #import the library\n",
    "porter=PorterStemmer() #create the object\n",
    "print(porter.stem('ran'))\n",
    "print(porter.stem('running'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70538f8e",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5fbb70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\RAVI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the libraries\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet') #download the wordnet database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a20f2390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouse\n",
      "wa\n",
      "be\n",
      "better\n",
      "going\n",
      "go\n",
      "go\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "lemmatizer =WordNetLemmatizer() #create the object\n",
    "print(lemmatizer.lemmatize('mice'))\n",
    "print(lemmatizer.lemmatize('was'))\n",
    "print(lemmatizer.lemmatize('was',pos=wordnet.VERB)) #pos->parts of speech,default is noun\n",
    "print(lemmatizer.lemmatize('better'))\n",
    "print(lemmatizer.lemmatize('going'))\n",
    "print(lemmatizer.lemmatize('going',pos=wordnet.VERB))\n",
    "print(lemmatizer.lemmatize('went',pos=wordnet.VERB))\n",
    "print(lemmatizer.lemmatize('better',pos=wordnet.ADJ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae7f30f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a725357a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\RAVI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger') #download the appropriate package for POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e15461c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"Donald Trump has a devoted following\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "451938bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Donald', 'NNP'),\n",
       " ('Trump', 'NNP'),\n",
       " ('has', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('devoted', 'VBN'),\n",
       " ('following', 'NN')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_and_tags=nltk.pos_tag(sentence) #get the NLTK given tags can get the lemmatizer version by above function\n",
    "words_and_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50019d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald\n",
      "Trump\n",
      "have\n",
      "a\n",
      "devote\n",
      "following\n"
     ]
    }
   ],
   "source": [
    "for word,tag in words_and_tags:\n",
    "    lemma=lemmatizer.lemmatize(word,pos=get_wordnet_pos(tag))\n",
    "    print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d62e2933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "cat\n",
      "be\n",
      "folowing\n",
      "the\n",
      "bird\n"
     ]
    }
   ],
   "source": [
    "sentence=\"The cat is folowing the bird\".split()\n",
    "words_and_tags=nltk.pos_tag(sentence)\n",
    "#print(words_and_tags)\n",
    "for word,tag in words_and_tags:\n",
    "    lemma=lemmatizer.lemmatize(word,pos=get_wordnet_pos(tag))\n",
    "    print(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee62e4a",
   "metadata": {},
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "906c196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB #classifier which selected(this can be changed)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03eb8557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\RAVI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RAVI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\RAVI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#downloads\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7aadb4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File 'bbc_text_cls.csv' already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#downlod the dataset\n",
    "!wget -nc https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eea37cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('bbc_text_cls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c28da01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    labels\n",
       "0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
       "1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
       "2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
       "3  High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
       "4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7880d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=df['text']\n",
    "labels=df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47dd4063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sport            511\n",
      "business         510\n",
      "politics         417\n",
      "tech             401\n",
      "entertainment    386\n",
      "Name: labels, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#for check wether the imbalance of the dataset\n",
    "labels.hist(figsize=(10,5))\n",
    "print(labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb064879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "inputs_train,inputs_test,Ytrain,Ytest=train_test_split(inputs,labels,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77e496f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1137    Whitehall shredding increasing - Tories\\n\\nCiv...\n",
       "1394    Greek duo cleared in doping case\\n\\nSprinters ...\n",
       "800     U2 stars enter rock Hall of Fame\\n\\nSinger Bru...\n",
       "2138    Warning over tsunami aid website\\n\\nNet users ...\n",
       "1535    Benitez delight after crucial win\\n\\nLiverpool...\n",
       "                              ...                        \n",
       "1147    Blair 'said he would stand down'\\n\\nTony Blair...\n",
       "2154    IBM puts cash behind Linux push\\n\\nIBM is spen...\n",
       "1766    Hingis hints at playing comeback\\n\\nMartina Hi...\n",
       "1122    Clarke faces ID cards rebellion\\n\\nCharles Cla...\n",
       "1346    Athens memories soar above lows\\n\\nWell, it's ...\n",
       "Name: text, Length: 1668, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25262432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1137         politics\n",
       "1394            sport\n",
       "800     entertainment\n",
       "2138             tech\n",
       "1535            sport\n",
       "            ...      \n",
       "1147         politics\n",
       "2154             tech\n",
       "1766            sport\n",
       "1122         politics\n",
       "1346            sport\n",
       "Name: labels, Length: 1668, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05ef16f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer() #create the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77c65ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain=vectorizer.fit_transform(inputs_train)\n",
    "Xtest=vectorizer.transform(inputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54926d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1668x26287 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 337411 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain #size of the matrix ->1668x26287"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a15eae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337411"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Xtrain!=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a276bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43846716"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(Xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77e1aee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007695239935415004"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Xtrain!=0).sum()/np.prod(Xtrain.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f60ddc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score:  0.9922062350119905\n",
      "test score:  0.9712746858168761\n"
     ]
    }
   ],
   "source": [
    "model=MultinomialNB()\n",
    "model.fit(Xtrain,Ytrain)\n",
    "print('train score: ',model.score(Xtrain,Ytrain))\n",
    "print('test score: ',model.score(Xtest,Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13c06708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score:  0.9928057553956835\n",
      "test score:  0.9766606822262118\n"
     ]
    }
   ],
   "source": [
    "#by removing stopwords\n",
    "vectorizer=CountVectorizer(stop_words='english') #create the object\n",
    "Xtrain=vectorizer.fit_transform(inputs_train)\n",
    "Xtest=vectorizer.transform(inputs_test)\n",
    "model=MultinomialNB()\n",
    "model.fit(Xtrain,Ytrain)\n",
    "print('train score: ',model.score(Xtrain,Ytrain))\n",
    "print('test score: ',model.score(Xtest,Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f187ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 391)\t1\n",
      "  (0, 1802)\t1\n",
      "  (0, 1846)\t4\n",
      "  (0, 2273)\t1\n",
      "  (0, 2389)\t1\n",
      "  (0, 2680)\t2\n",
      "  (0, 2970)\t1\n",
      "  (0, 4906)\t1\n",
      "  (0, 5310)\t1\n",
      "  (0, 5701)\t1\n",
      "  (0, 5808)\t1\n",
      "  (0, 6514)\t1\n",
      "  (0, 7286)\t1\n",
      "  (0, 7709)\t1\n",
      "  (0, 8025)\t8\n",
      "  (0, 8076)\t1\n",
      "  (0, 9127)\t1\n",
      "  (0, 9208)\t1\n",
      "  (0, 9220)\t1\n",
      "  (0, 9313)\t1\n",
      "  (0, 9584)\t1\n",
      "  (0, 9783)\t2\n",
      "  (0, 10422)\t1\n",
      "  (0, 10780)\t1\n",
      "  (0, 10871)\t1\n",
      "  :\t:\n",
      "  (556, 19444)\t1\n",
      "  (556, 19708)\t1\n",
      "  (556, 19763)\t1\n",
      "  (556, 20294)\t1\n",
      "  (556, 20391)\t2\n",
      "  (556, 20412)\t1\n",
      "  (556, 20668)\t1\n",
      "  (556, 20934)\t1\n",
      "  (556, 21935)\t1\n",
      "  (556, 21987)\t1\n",
      "  (556, 21989)\t1\n",
      "  (556, 22006)\t1\n",
      "  (556, 22579)\t1\n",
      "  (556, 22668)\t1\n",
      "  (556, 23513)\t1\n",
      "  (556, 23568)\t1\n",
      "  (556, 23588)\t1\n",
      "  (556, 24166)\t1\n",
      "  (556, 24702)\t1\n",
      "  (556, 25087)\t2\n",
      "  (556, 25375)\t1\n",
      "  (556, 25686)\t1\n",
      "  (556, 25702)\t1\n",
      "  (556, 25853)\t3\n",
      "  (556, 25857)\t1 717     entertainment\n",
      "798     entertainment\n",
      "1330            sport\n",
      "18           business\n",
      "1391            sport\n",
      "            ...      \n",
      "1636            sport\n",
      "1422            sport\n",
      "1982             tech\n",
      "2005             tech\n",
      "550     entertainment\n",
      "Name: labels, Length: 557, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(Xtest,Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b2aed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "854beb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do all the works in tokenizing and lemmatizing each document\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl=WordNetLemmatizer() #create the object\n",
    "    def __call__(self,doc): #take 1 argument(document to tokenize)\n",
    "        tokens=word_tokenize(doc) #convert document into tokens\n",
    "        words_and_tags=nltk.pos_tag(tokens) #objain the POS tag\n",
    "        return [self.wnl.lemmatize(word,pos=get_wordnet_pos(tag)) for word,tag in words_and_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f13e9b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score:  0.9922062350119905\n",
      "test score:  0.9676840215439856\n"
     ]
    }
   ],
   "source": [
    "vectorizer=CountVectorizer(tokenizer=LemmaTokenizer()) #with custom tokenizer\n",
    "Xtrain=vectorizer.fit_transform(inputs_train)\n",
    "Xtest=vectorizer.transform(inputs_test)\n",
    "model=MultinomialNB()\n",
    "model.fit(Xtrain,Ytrain)\n",
    "print('train score: ',model.score(Xtrain,Ytrain))\n",
    "print('test score: ',model.score(Xtest,Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e06edb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do all the works in tokenizing and stemming each document\n",
    "class StemTokenizer:\n",
    "    def __init__(self):\n",
    "        self.porter=PorterStemmer() #create the object\n",
    "    def __call__(self,doc): #take 1 argument(document to tokenize)\n",
    "        tokens=word_tokenize(doc) #convert document into tokens\n",
    "        return [self.porter.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8eaf256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score:  0.9892086330935251\n",
      "test score:  0.9694793536804309\n"
     ]
    }
   ],
   "source": [
    "vectorizer=CountVectorizer(tokenizer=StemTokenizer()) #with custom tokenizer\n",
    "Xtrain=vectorizer.fit_transform(inputs_train)\n",
    "Xtest=vectorizer.transform(inputs_test)\n",
    "model=MultinomialNB()\n",
    "model.fit(Xtrain,Ytrain)\n",
    "print('train score: ',model.score(Xtrain,Ytrain))\n",
    "print('test score: ',model.score(Xtest,Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2fc03d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(s):\n",
    "    return s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac14b882",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m vectorizer\u001b[38;5;241m=\u001b[39mCountVectorizer(tokenizer\u001b[38;5;241m=\u001b[39msimple_tokenizer) \u001b[38;5;66;03m#with custom tokenizer\u001b[39;00m\n\u001b[0;32m      2\u001b[0m Xtrain\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mfit_transform(inputs_train)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mXtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m Xtest\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mtransform(inputs_test)\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m=\u001b[39mMultinomialNB()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "vectorizer=CountVectorizer(tokenizer=simple_tokenizer) #with custom tokenizer\n",
    "Xtrain=vectorizer.fit_transform(inputs_train)\n",
    "print(Xtrain.shape())\n",
    "Xtest=vectorizer.transform(inputs_test)\n",
    "model=MultinomialNB()\n",
    "model.fit(Xtrain,Ytrain)\n",
    "print('train score: ',model.score(Xtrain,Ytrain))\n",
    "print('test score: ',model.score(Xtest,Ytest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
